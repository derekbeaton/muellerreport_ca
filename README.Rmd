---
title: "Correspondence analyses of the Mueller Report"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(ExPosition)
  # if needed.
# devtools::install_github("derekbeaton/GSVD")
# devtools::install_github("derekbeaton/OuRS", subdir = "/OuRS")
library(GSVD)
library(ours)

library(ggplot2)
```

# Background

SOmething

# Explanation of the directory structure


# Cleaning and preprocessing

For processing of the text, I did little additional work on top of two analyses already performed: [batpigandme/tidymueller](https://github.com/batpigandme/tidymueller) and [cbail/mueller_report](https://github.com/cbail/mueller_report). A few extra cleaning steps were performed that align with [some of our previous work on NeuroSynth](https://www.biorxiv.org/content/10.1101/157826v3), and that code can be found in [fahd09/neurosynth_semantic_map](https://github.com/fahd09/neurosynth_semantic_map).

# Basics of the analyses

In principle, these analyses are virtually the same as what we did for the NeuroSynth database (see [our paper on correspondence analysis of manuscript-by-terms in the NeuroSynth database](https://www.biorxiv.org/content/10.1101/157826v3) and [fahd09/neurosynth_semantic_map](https://github.com/fahd09/neurosynth_semantic_map)).

# Correspondence analyses of the Mueller Report



```{r load_data_run_cas}

page_by_word_counts.matrix_for_analyses <- read.csv(here::here("data","for_analyses","page_by_word_counts.matrix_for_analyses.csv"))
page_by_word.matrix_for_analyses <- read.csv(here::here("data","for_analyses","page_by_word.matrix_for_analyses.csv"))

page_by_lemma_counts.matrix_for_analyses <- read.csv(here::here("data","for_analyses","page_by_lemma_counts.matrix_for_analyses.csv"))
page_by_lemma.matrix_for_analyses <- read.csv(here::here("data","for_analyses","page_by_lemma.matrix_for_analyses.csv"))

words_counts_ca <- epCA(page_by_word_counts.matrix_for_analyses, graphs=F)
words_ca <- epCA(page_by_word.matrix_for_analyses, graphs=F)

lemmas_counts_ca <- epCA(page_by_word_counts.matrix_for_analyses, graphs=F)
lemmas_ca <- epCA(page_by_word.matrix_for_analyses, graphs=F)

```


```{r summaries_diagnoistics}


```


```{r visualize_components}


```


```{r comparisons}

## corrplots or similar heatmaps?


```

